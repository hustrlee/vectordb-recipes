{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从头构建一个 RAG 系统，适用于中文\n",
    "\n",
    "原始教程是对一段英文材料做 RAG Pipeline。使用的方法和工具链并不完全适用于中文 RAG。根据原教程提供的整体思路，这里从头实现了一个中文 RAG 系统，并将其封装成了几乎满足生成环境的类。\n",
    "\n",
    "## 整体架构及工具链\n",
    "\n",
    "1. 使用 `lease.txt` 的中文翻译 `lease-zh.txt` 作为语料。\n",
    "\n",
    "2. 使用 GTE Embedding 的 `Autotokenizer` 功能实现精确地 chunk 分割。\n",
    "3. 使用 `thenlper/gte-large-zh` 实现嵌入（embedding）。\n",
    "4. 使用 lancedb 作为矢量存储。\n",
    "5. `create_fts_index` 和 `create_index` 来加速全文搜索和矢量搜索。\n",
    "6. 使用 lancedb 的 `search` 实现全文搜索和矢量搜索（retriever）。\n",
    "7. 使用 `BAAI/bge-reranker-base` 实现重排（Rerank）。\n",
    "8. 使用火山引擎部署的 DeepSeek V1 实现答案生成（Answer Generator）。\n",
    "9. 使用 TEI 部署 `gte-large-zh` 和 `bge-reranker-base`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: 合同是什么时候签署的？\n",
      "\n",
      "相关内容:\n",
      "- 年 12 月 1 日 。 第 三 条 ： 续 期 本 协 议 双 方 可 选 择 续 签 ， 并 就 续 签 的 条 款 和 条 件 另 行 书 面 达 成 一 致 并 签 署 文 件 。 第 四 条 ： 租 金 确 定 第 1 节 ： 月 租 金 承 租 方 同 意 在 租 赁 期 内 按 月 向 出 租 方 支 付 租 金 ， 每 月 金 额 为 40, 000 美 元 ， 付 款 地 点 由 出 租 方 书 面 通 知 确 定 。 第 2 节 ： 逾 期 费 用 若 月 租 金 未 在 每 月 第 十 日 之 前 （ 含 ） 邮 寄 或 被 出 租 方 收 到 ， 将 收 取 5 % 的 滞 纳 金 。 第 五 条 ： 保 证 金 承 租 方 已 向 出 租 方 支 付 20, 000 美 元 作 为 履 行 本 协 议 条 款 的 保 证 金 。 如 承 租 方 在 租 期 满 后 完 全 履 行 其 义 务 ， 则 该 金 额 将 全 额 退 还 。 若 租 赁 物 业 被 真 实 出 售 ， 出 租 方 有 权 将 该 保 证 金 转 交 买 方 ， 并 解 除 其 归 还 义 务 。 第 六 条 ： 税\n",
      "  (相关度: 0.230)\n",
      "- 年 12 月 1 日 。 第 三 条 ： 续 期 本 协 议 双 方 可 选 择 续 签 ， 并 就 续 签 的 条 款 和 条 件 另 行 书 面 达 成 一 致 并 签 署 文 件 。 第 四 条 ： 租 金 确 定 第 1 节 ： 月 租 金 承 租 方 同 意 在 租 赁 期 内 按 月 向 出 租 方 支 付 租 金 ， 每 月 金 额 为 40, 000 美 元 ， 付 款 地 点 由 出 租 方 书 面 通 知 确 定 。 第 2 节 ： 逾 期 费 用 若 月 租 金 未 在 每 月 第 十 日 之 前 （ 含 ） 邮 寄 或 被 出 租 方 收 到 ， 将 收 取 5 % 的 滞 纳 金 。 第 五 条 ： 保 证 金 承 租 方 已 向 出 租 方 支 付 20, 000 美 元 作 为 履 行 本 协 议 条 款 的 保 证 金 。 如 承 租 方 在 租 期 满 后 完 全 履 行 其 义 务 ， 则 该 金 额 将 全 额 退 还 。 若 租 赁 物 业 被 真 实 出 售 ， 出 租 方 有 权 将 该 保 证 金 转 交 买 方 ， 并 解 除 其 归 还 义 务 。 第 六 条 ： 税\n",
      "  (相关度: 0.230)\n",
      "- 年 12 月 1 日 。 第 三 条 ： 续 期 本 协 议 双 方 可 选 择 续 签 ， 并 就 续 签 的 条 款 和 条 件 另 行 书 面 达 成 一 致 并 签 署 文 件 。 第 四 条 ： 租 金 确 定 第 1 节 ： 月 租 金 承 租 方 同 意 在 租 赁 期 内 按 月 向 出 租 方 支 付 租 金 ， 每 月 金 额 为 40, 000 美 元 ， 付 款 地 点 由 出 租 方 书 面 通 知 确 定 。 第 2 节 ： 逾 期 费 用 若 月 租 金 未 在 每 月 第 十 日 之 前 （ 含 ） 邮 寄 或 被 出 租 方 收 到 ， 将 收 取 5 % 的 滞 纳 金 。 第 五 条 ： 保 证 金 承 租 方 已 向 出 租 方 支 付 20, 000 美 元 作 为 履 行 本 协 议 条 款 的 保 证 金 。 如 承 租 方 在 租 期 满 后 完 全 履 行 其 义 务 ， 则 该 金 额 将 全 额 退 还 。 若 租 赁 物 业 被 真 实 出 售 ， 出 租 方 有 权 将 该 保 证 金 转 交 买 方 ， 并 解 除 其 归 还 义 务 。 第 六 条 ： 税\n",
      "  (相关度: 0.230)\n",
      "- 年 12 月 1 日 。 第 三 条 ： 续 期 本 协 议 双 方 可 选 择 续 签 ， 并 就 续 签 的 条 款 和 条 件 另 行 书 面 达 成 一 致 并 签 署 文 件 。 第 四 条 ： 租 金 确 定 第 1 节 ： 月 租 金 承 租 方 同 意 在 租 赁 期 内 按 月 向 出 租 方 支 付 租 金 ， 每 月 金 额 为 40, 000 美 元 ， 付 款 地 点 由 出 租 方 书 面 通 知 确 定 。 第 2 节 ： 逾 期 费 用 若 月 租 金 未 在 每 月 第 十 日 之 前 （ 含 ） 邮 寄 或 被 出 租 方 收 到 ， 将 收 取 5 % 的 滞 纳 金 。 第 五 条 ： 保 证 金 承 租 方 已 向 出 租 方 支 付 20, 000 美 元 作 为 履 行 本 协 议 条 款 的 保 证 金 。 如 承 租 方 在 租 期 满 后 完 全 履 行 其 义 务 ， 则 该 金 额 将 全 额 退 还 。 若 租 赁 物 业 被 真 实 出 售 ， 出 租 方 有 权 将 该 保 证 金 转 交 买 方 ， 并 解 除 其 归 还 义 务 。 第 六 条 ： 税\n",
      "  (相关度: 0.230)\n",
      "- 年 12 月 1 日 。 第 三 条 ： 续 期 本 协 议 双 方 可 选 择 续 签 ， 并 就 续 签 的 条 款 和 条 件 另 行 书 面 达 成 一 致 并 签 署 文 件 。 第 四 条 ： 租 金 确 定 第 1 节 ： 月 租 金 承 租 方 同 意 在 租 赁 期 内 按 月 向 出 租 方 支 付 租 金 ， 每 月 金 额 为 40, 000 美 元 ， 付 款 地 点 由 出 租 方 书 面 通 知 确 定 。 第 2 节 ： 逾 期 费 用 若 月 租 金 未 在 每 月 第 十 日 之 前 （ 含 ） 邮 寄 或 被 出 租 方 收 到 ， 将 收 取 5 % 的 滞 纳 金 。 第 五 条 ： 保 证 金 承 租 方 已 向 出 租 方 支 付 20, 000 美 元 作 为 履 行 本 协 议 条 款 的 保 证 金 。 如 承 租 方 在 租 期 满 后 完 全 履 行 其 义 务 ， 则 该 金 额 将 全 额 退 还 。 若 租 赁 物 业 被 真 实 出 售 ， 出 租 方 有 权 将 该 保 证 金 转 交 买 方 ， 并 解 除 其 归 还 义 务 。 第 六 条 ： 税\n",
      "  (相关度: 0.230)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoTokenizer\n",
    "import lancedb\n",
    "import requests\n",
    "import uuid\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def tei_embed(texts: List[str], url: str) -> List[List[float]]:\n",
    "    \"\"\"调用 TEI 服务获取文本嵌入向量,添加重试机制\"\"\"\n",
    "    response = requests.post(url, json={\"inputs\": texts}, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def tei_rerank(query: str, passages: List[str], url: str) -> List[Dict]:\n",
    "    \"\"\"调用 TEI 服务对文本进行重排序,添加重试机制\"\"\"\n",
    "    response = requests.post(url, json={\"query\": query, \"texts\": passages}, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    return [r[\"score\"] for r in response.json()]\n",
    "\n",
    "\n",
    "class Document(LanceModel):\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    vector: Vector(1024)  # type: ignore\n",
    "    # metadata: dict\n",
    "\n",
    "\n",
    "class GTEChunkedRAGPipeline:\n",
    "    \"\"\"使用 GTE 分块的 RAG Pipeline,调用 TEI 服务进行 embedding 和 rerank\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"http://localhost:8081/embed\",\n",
    "        reranker_model: str = \"http://localhost:8082/rerank\",\n",
    "        lancedb_uri: str = \"./rag_lancedb\",\n",
    "        table_name: str = \"documents\",\n",
    "        chunk_token_limit: int = 256,\n",
    "        chunk_overlap: int = 50,\n",
    "        max_batch_tokens: int = 8192,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化 RAG Pipeline\n",
    "\n",
    "        Args:\n",
    "            embedding_model: embedding 服务地址\n",
    "            reranker_model: reranker 服务地址\n",
    "            lancedb_uri: lancedb 数据库地址\n",
    "            table_name: 表名\n",
    "            chunk_token_limit: 每个文本块的最大 token 数\n",
    "            chunk_overlap: 文本块之间的重叠 token 数\n",
    "            max_batch_tokens: 批处理的最大 token 数\n",
    "        \"\"\"\n",
    "        self.embedding_url = embedding_model\n",
    "        self.reranker_url = reranker_model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-large-zh\")\n",
    "\n",
    "        self.db = lancedb.connect(lancedb_uri)\n",
    "        self.table = (\n",
    "            self.db.open_table(table_name)\n",
    "            if table_name in self.db.table_names()\n",
    "            else self.db.create_table(table_name, schema=Document)\n",
    "        )\n",
    "\n",
    "        self.chunk_token_limit = chunk_token_limit\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.max_batch_tokens = max_batch_tokens\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"将文本分割成块\"\"\"\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(tokens):\n",
    "            end = start + self.chunk_token_limit\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_text.strip())  # 去除首尾空白\n",
    "\n",
    "            if end >= len(tokens):\n",
    "                break\n",
    "            start += self.chunk_token_limit - self.chunk_overlap\n",
    "\n",
    "        return [c for c in chunks if c]  # 过滤空字符串\n",
    "\n",
    "    def embed_chunks(self, chunks: List[str]) -> List[List[float]]:\n",
    "        \"\"\"对文本块进行向量化\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "\n",
    "        # 分批控制总 token 数量\n",
    "        batches = []\n",
    "        batch, token_count = [], 0\n",
    "\n",
    "        for chunk in chunks:\n",
    "            tokens = self.tokenizer.encode(chunk, add_special_tokens=False)\n",
    "            if token_count + len(tokens) > self.max_batch_tokens and batch:\n",
    "                batches.append(batch)\n",
    "                batch, token_count = [], 0\n",
    "            batch.append(chunk)\n",
    "            token_count += len(tokens)\n",
    "\n",
    "        if batch:\n",
    "            batches.append(batch)\n",
    "\n",
    "        embeddings = []\n",
    "        for batch in batches:\n",
    "            batch_embeddings = tei_embed(batch, self.embedding_url)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None):\n",
    "        \"\"\"添加文本到数据库\"\"\"\n",
    "        if not texts:\n",
    "            return\n",
    "\n",
    "        metadatas = metadatas or [{}] * len(texts)\n",
    "        to_insert = []\n",
    "\n",
    "        for i, (text, metadata) in enumerate(zip(texts, metadatas)):\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            chunks = self.chunk_text(text)\n",
    "            embeddings = self.embed_chunks(chunks)\n",
    "\n",
    "            for idx, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
    "                to_insert.append(\n",
    "                    {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_id\": f\"{doc_id}_{idx}\",\n",
    "                        \"text\": chunk,\n",
    "                        \"vector\": emb,\n",
    "                        # **metadata,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if to_insert:\n",
    "            self.table.add(to_insert)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"搜索相似文本\"\"\"\n",
    "        # 获取查询向量\n",
    "        query_embedding = tei_embed([query], self.embedding_url)[0]\n",
    "\n",
    "        # 向量搜索\n",
    "        results = self.table.search(query_embedding).limit(top_k * 3).to_pandas()\n",
    "        if results.empty:\n",
    "            return []\n",
    "\n",
    "        # 重排序\n",
    "        texts = results[\"text\"].tolist()\n",
    "        rerank_scores = tei_rerank(query, texts, self.reranker_url)\n",
    "\n",
    "        # 合并结果并排序\n",
    "        sorted_results = sorted(\n",
    "            zip(results.to_dict(\"records\"), rerank_scores),\n",
    "            key=lambda x: x[1][\"score\"] if isinstance(x[1], dict) else x[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"text\": r[\"text\"],\n",
    "                \"score\": s[\"score\"] if isinstance(s, dict) else s,\n",
    "                \"doc_id\": r.get(\"doc_id\"),\n",
    "                \"chunk_id\": r.get(\"chunk_id\"),\n",
    "            }\n",
    "            for r, s in sorted_results[:top_k]\n",
    "        ]\n",
    "\n",
    "\n",
    "# 创建 RAG 实例\n",
    "rag = GTEChunkedRAGPipeline()\n",
    "\n",
    "# 读取租赁合同文本\n",
    "with open(\"lease-zh.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lease_text = f.read()\n",
    "\n",
    "# 添加文档到向量数据库\n",
    "rag.add_texts([lease_text], [{\"source\": \"lease-zh.txt\"}])\n",
    "\n",
    "# 提问\n",
    "query = \"合同是什么时候签署的？\"\n",
    "results = rag.search(query)\n",
    "\n",
    "print(f\"问题: {query}\\n\")\n",
    "print(\"相关内容:\")\n",
    "for r in results:\n",
    "    print(f\"- {r['text']}\\n  (相关度: {r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ARK_API_KEY=d04594ba-8fd8-4849-9e71-8734e4bf45f3\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (/home/rlee/.miniconda3/envs/vdb/lib/python3.11/site-packages/openai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33menv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mARK_API_KEY=d04594ba-8fd8-4849-9e71-8734e4bf45f3\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# gets API Key from environment variable OPENAI_API_KEY\u001b[39;00m\n\u001b[32m      7\u001b[39m client = OpenAI(\n\u001b[32m      8\u001b[39m     api_key=os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mARK_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      9\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://ark.cn-beijing.volces.com/api/v3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'OpenAI' from 'openai' (/home/rlee/.miniconda3/envs/vdb/lib/python3.11/site-packages/openai/__init__.py)"
     ]
    }
   ],
   "source": [
    "%env ARK_API_KEY=d04594ba-8fd8-4849-9e71-8734e4bf45f3\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# gets API Key from environment variable OPENAI_API_KEY\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"ARK_API_KEY\"),\n",
    "    base_url=\"https://ark.cn-beijing.volces.com/api/v3\",\n",
    ")\n",
    "\n",
    "# Context Prompt\n",
    "\n",
    "base_prompt = \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer using the provided contexts. Every answer you generate should have citations in this pattern  \"Answer [position].\", for example: \"Earth is round [1][2].,\" if it's relevant.\n",
    "\n",
    "Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state, \"The provided context does not have the answer.\"\n",
    "\n",
    "User question: {}\n",
    "\n",
    "Contexts:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "# Your prompt\n",
    "prompt = f\"{base_prompt.format(query, [r['text'] for r in results])}\"\n",
    "\n",
    "# 替换成 Doubao\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=0,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-v3-250324\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
